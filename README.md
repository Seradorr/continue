# Continue AI Configuration - Professional Local LLM Setup

**Version**: 7.0.0
**Last Updated**: 2026-02-24

Lokal olarak Ã§alÄ±ÅŸan LLM'leri (GLM-5, Kimi-K2.5, Qwen3) GitHub Copilot seviyesinde kod Ã¼retimi, debugging, refactoring ve dokÃ¼mantasyon iÃ§in optimize eden Continue IDE eklentisi yapÄ±landÄ±rmasÄ±.

---

## ğŸ¯ AmaÃ§

Bu proje, **lokal LLM'lerin profesyonel yazÄ±lÄ±m geliÅŸtirme asistanÄ± olarak kullanÄ±lmasÄ±nÄ±** saÄŸlar:

| Hedef | AÃ§Ä±klama |
|-------|----------|
| **GitHub Copilot Seviyesi** | Kod tamamlama, debugging, refactoring kalitesi |
| **UzmanlÄ±k AlanlarÄ±** | FPGA/RTL, Embedded C, C#/.NET, Python |
| **Mimari DanÄ±ÅŸmanlÄ±k** | Proje analizi, pattern tespiti, iyileÅŸtirme Ã¶nerileri |
| **DokÃ¼mantasyon** | README, Changelog, API docs oluÅŸturma/gÃ¼ncelleme |
| **Åematik/GÃ¶rsel Analiz** | Devre ÅŸemasÄ±, blok diyagram, Excel pin mapping okuma |
| **Versiyon Kontrol** | Git operasyonlarÄ±, commit analizi, branch yÃ¶netimi |

---

## ğŸ—ï¸ Mimari

### Model YapÄ±landÄ±rmasÄ±

| # | Model | Rol | Context | KullanÄ±m AlanÄ± |
|---|-------|-----|---------|----------------|
| 1-4 | **GLM-5-FP8** | UzmanlÄ±k modelleri | 131K | FPGA, Vitis, C#, Python |
| 5-9 | **Kimi-K2.5** | UzmanlÄ±k modelleri | 128K | FPGA, Vitis, C#, Python, Docs |
| 10 | **Kimi-K2.5** | Schematic-Engineer | 128K | Åematik/gÃ¶rsel okuma (image_input) |
| 11 | **Kimi-K2.5** | Git-Expert | 128K | Versiyon kontrol yÃ¶netimi |
| 12 | **Qwen3-Next-80B** | Quick-Engineer | 262K | HÄ±zlÄ± temel iÅŸler (3B aktif MoE) |
| 13 | **Qwen3-Coder-480B** | Rerank | 131K | Arama sonucu sÄ±ralama |
| 14 | **Qwen3-Embedding-8B** | Embed | - | VektÃ¶rleÅŸtirme |

### GLM-5-FP8 Bilgileri

| Ã–zellik | DeÄŸer |
|---------|-------|
| Toplam Parametre | 744B |
| Aktif Parametre | 40B (MoE, 256 expert) |
| Mimari | GlmMoeDsaForCausalLM (MoE + Sparse MLA / DSA) |
| Pre-training | 28.5T token |
| vLLM Parser | `--tool-call-parser glm47 --reasoning-parser glm45` |
| GPU Gereksinimi | H100/H200+ (sm90+, A100 desteklenmiyor) |
| MTP (Speculative) | Tool calling ile uyumsuz â€” kapalÄ± tutun |

### Kimi-K2.5 Bilgileri

| Ã–zellik | DeÄŸer |
|---------|-------|
| Thinking Mode | AKTIF (varsayÄ±lan, kapatmayÄ±n â€” model kalitesi dÃ¼ÅŸer) |
| Temperature | 1.0 (Moonshot resmi Ã¶nerisi, thinking ON) |
| vLLM Parser | `--tool-call-parser kimi_k2 --reasoning-parser kimi_k2` |
| reasoning_effort | Desteklenmiyor (binary ON/OFF) |
| Bilinen Sorun | Tool token leakage (vLLM PR #34955 bekliyor) |

### Rules HiyerarÅŸisi

```
.continue/rules/
â”œâ”€â”€ 00-core.md          # Temel protokol (alwaysApply)
â”œâ”€â”€ 01-general.md       # Genel mÃ¼hendislik (alwaysApply)
â”œâ”€â”€ 02-fpga.md          # VHDL/Verilog (glob: *.vhd, *.v, *.sv)
â”œâ”€â”€ 03-vitis.md         # Embedded C (glob: *.c, *.h)
â”œâ”€â”€ 04-csharp.md        # C#/.NET (glob: *.cs, *.xaml)
â”œâ”€â”€ 05-python.md        # Python (glob: *.py)
â”œâ”€â”€ 06-documentation.md # Markdown (glob: *.md)
â””â”€â”€ 07-reasoning.md     # GeliÅŸmiÅŸ akÄ±l yÃ¼rÃ¼tme (alwaysApply)
```

---

## ğŸ”‘ Temel Ã–zellikler

### 1. UzmanlÄ±k Modelleri
Her alan iÃ§in optimize edilmiÅŸ system message ve agent prompt:
- **FPGA-RTL-Engineer**: FSM, sentez, timing, CDC, AXI, Vivado
- **Embedded-C-Cpp-Vitis**: Zynq, BSP, DMA, ISR, cache
- **CSharp-DotNet-Engineer**: async/await, MVVM, WPF, SOLID
- **Python-Engineer**: PEP 8, type hints, pathlib, pytest
- **Schematic-Engineer**: Devre ÅŸemasÄ±, blok diyagram, Excel, image_input
- **Git-Expert**: Commit, branch, merge, rebase, conflict resolution

### 2. Testbench AyrÄ±mÄ±
RTL dosyalarÄ±na testbench otomatik **eklenmez**:
- Sadece `module.vhd` istenirse â†’ sadece `module.vhd`
- Testbench istenirse â†’ ayrÄ± `module_tb.vhd` dosyasÄ±

### 3. DokÃ¼mantasyon KorumasÄ±
Markdown dosyalarÄ±nda mevcut iÃ§erik **silinmez**:
- "Ekle" = mevcut + yeni
- Changelog yeni giriÅŸ = en Ã¼ste
- Format (baÅŸlÄ±k, liste, tablo) korunur

### 4. Tekrar YasaÄŸÄ± ve Loop Ã–nleme
- AynÄ± cÃ¼mle farklÄ± kelimelerle **yasak**
- Dolgu kelimeleri yasak (Hmm, Let me think, Wait)
- Adaptif dÃ¼ÅŸÃ¼nme: basit istek 3-5 adÄ±m, derin analiz 8-12 adÄ±m
- Atomik Ã§Ä±ktÄ±: tÃ¼m deÄŸiÅŸiklikler tek yanÄ±tta

### 5. Error Recovery
Ä°ÅŸlem baÅŸarÄ±sÄ±z olursa:
- BaÅŸarÄ±lÄ± adÄ±mÄ± koru
- Alternatif strateji dene
- KullanÄ±cÄ±ya net bilgi ver

---

## ğŸ“ Dosya YapÄ±sÄ±

```
.
â”œâ”€â”€ config.yaml              # Ana yapÄ±landÄ±rma dosyasÄ±
â”œâ”€â”€ .continue/
â”‚   â””â”€â”€ rules/
â”‚       â”œâ”€â”€ 00-core.md       # Temel agent protokolÃ¼
â”‚       â”œâ”€â”€ 01-general.md    # Genel mÃ¼hendislik kurallarÄ±
â”‚       â”œâ”€â”€ 02-fpga.md       # FPGA/RTL kurallarÄ±
â”‚       â”œâ”€â”€ 03-vitis.md      # Vitis/Embedded C kurallarÄ±
â”‚       â”œâ”€â”€ 04-csharp.md     # C#/.NET kurallarÄ±
â”‚       â”œâ”€â”€ 05-python.md     # Python kurallarÄ±
â”‚       â”œâ”€â”€ 06-documentation.md # DokÃ¼mantasyon kurallarÄ±
â”‚       â””â”€â”€ 07-reasoning.md  # GeliÅŸmiÅŸ akÄ±l yÃ¼rÃ¼tme
â””â”€â”€ README.md                # Bu dosya
```

---

## âš™ï¸ Kurulum

1. **Continue eklentisini** VS Code'a kurun
2. Bu repository'yi klonlayÄ±n:
   ```bash
   git clone https://github.com/Seradorr/continue.git
   ```
3. `config.yaml` iÃ§indeki API ayarlarÄ±nÄ± dÃ¼zenleyin:
   ```yaml
   api_config: &api_config
     provider: openai
     apiBase: "http://your-api-base:8000/v1"
     apiKey: "your-api-key"
   ```
4. `.continue/rules/` klasÃ¶rÃ¼nÃ¼ VS Code workspace'inize kopyalayÄ±n

### vLLM Sunucu YapÄ±landÄ±rmasÄ±

**GLM-5-FP8:**
```bash
vllm serve GLM-5-FP8 \
  --tensor-parallel-size 8 \
  --gpu-memory-utilization 0.85 \
  --tool-call-parser glm47 \
  --reasoning-parser glm45 \
  --enable-auto-tool-choice
# NOT: --speculative-config (MTP) tool calling ile uyumsuz, eklemeyin
```

**Kimi-K2.5:**
```bash
vllm serve Kimi-K2.5 \
  --tool-call-parser kimi_k2 \
  --reasoning-parser kimi_k2 \
  --enable-auto-tool-choice
# Thinking mode varsayÄ±lan AÃ‡IK, kapatmayÄ±n
```

---

## ğŸ”§ YapÄ±landÄ±rma SeÃ§enekleri

### Temperature AyarlarÄ±

| Model | Temperature | Neden |
|-------|-------------|-------|
| GLM-5 (TÃ¼m uzmanlÄ±klar) | 0.7 | Z.ai resmi tool-calling Ã¶nerisi |
| Kimi-K2.5 (Thinking ON) | 1.0 | Moonshot resmi Ã¶nerisi |
| Quick-Engineer (Qwen3) | 0.5 | HÄ±zlÄ±, dengeli Ã§Ä±ktÄ± |
| Rerank | 0.1 | TutarlÄ± sÄ±ralama |

### Sampling Parametreleri

**GLM-5 (vLLM):**

| Parametre | DeÄŸer | AÃ§Ä±klama |
|-----------|-------|----------|
| `frequency_penalty` | 0.1 | Tekrar eden token'lara hafif ceza |
| `presence_penalty` | 0.1 | Yeni token Ã§eÅŸitliliÄŸi |
| `top_p` | 0.95 | Neredeyse tÃ¼m tokenler dahil |
| `repetition_penalty` | 1.1 | Loop Ã¶nleme |
| `truncate_prompt_tokens` | 98000 | Context overflow Ã¶nleme |

**Kimi-K2.5:**

| Parametre | DeÄŸer | AÃ§Ä±klama |
|-----------|-------|----------|
| `top_p` | 0.95 | Moonshot resmi Ã¶nerisi |
| `truncate_prompt_tokens` | 97000 | 131K - 32K = ~98K, gÃ¼venli 97K |

### Token Limitleri

| Model | contextLength | maxTokens | truncate |
|-------|---------------|-----------|----------|
| GLM-5-FP8 | 131072 | 32768 | 98000 |
| Kimi-K2.5 | 131072 | 32768 | 97000 |
| Qwen3-Next-80B | 262144 | 32768 | 229000 |

> **Context Overflow KorumasÄ±:** `truncate_prompt_tokens` ile input otomatik kesilir, session dÃ¼ÅŸmesi Ã¶nlenir.

---

## ğŸ› YaÅŸanan Sorunlar ve Ã‡Ã¶zÃ¼mler

### GLM-4.7 â†’ GLM-5 GeÃ§iÅŸi

GLM-5 (744B/40B MoE) Z.ai tarafÄ±ndan yayÄ±nlandÄ±. Ã–nemli notlar:
- vLLM v0.16+ gerekli
- Parser isimleri geriye uyumlu: `glm47`, `glm45`
- MTP (speculative decode) tool calling ile **uyumsuz** â€” kapalÄ± tutun
- H100/H200+ GPU gerekli (sm90+)

### GLM Sonsuz DÃ¼ÅŸÃ¼nme DÃ¶ngÃ¼sÃ¼ (Infinite Thinking Loop)

**Sorun:** GLM modeli thinking modunda sonsuz dÃ¶ngÃ¼ye giriyordu.

**Ã‡Ã¶zÃ¼m:**
- `repetition_penalty: 1.1` â€” loop Ã¶nleme
- `frequency_penalty: 0.1` â€” tekrar eden token cezasÄ±
- Prompt kurallarÄ±: dolgu kelimeleri yasak, tekrar yasaÄŸÄ±
- Adaptif dÃ¼ÅŸÃ¼nme limitleri (basit: 3-5, derin: 8-12 adÄ±m)

### Kimi K2.5 Tool Token Leakage

**Sorun:** `<|tool_call_begin|> ... <|tool_call_end|>` gibi Ã¶zel tokenler Ã§Ä±ktÄ±ya sÄ±zÄ±yor.

**Durum:**
- PR #28543 (KasÄ±m 2025, merged): Streaming mode sÄ±zÄ±ntÄ±sÄ± dÃ¼zeltildi
- PR #34955 (Åubat 2026, aÃ§Ä±k): KimiK25ReasoningParser â€” thinkingâ†’tool geÃ§iÅŸi
- PR #34968 (Åubat 2026, aÃ§Ä±k): 8K buffer limiti kaldÄ±rma

**Workaround:** vLLM'i en gÃ¼ncel sÃ¼rÃ¼me gÃ¼ncelleyin. Thinking modu **kapatmayÄ±n** â€” model kalitesi dÃ¼ÅŸer.

### Premature Close (BaÄŸlantÄ± KopmasÄ±)

**Sorun:** API baÄŸlantÄ±sÄ± erken kapanÄ±yordu.

**SonuÃ§:** Backend kaynaklÄ± sorun olarak tespit edilip Ã§Ã¶zÃ¼ldÃ¼.

---

## ğŸ“‹ Versiyon GeÃ§miÅŸi

### v7.0.0 (2026-02-24)
- **BREAKING:** GLM-4.7-FP8 â†’ **GLM-5-FP8** geÃ§iÅŸi (744B/40B MoE, DSA)
- **Kimi-K2.5 Context:** 262K â†’ **128K** (sunucu konfigÃ¼rasyonu)
- **Kimi-K2.5 Thinking:** temperature=1.0 (Moonshot resmi), thinking AÃ‡IK
- **Kimi-K2.5 maxTokens:** 65536 â†’ **32768**
- **Yeni Model:** Schematic-Engineer (Kimi-K2.5, image_input, ÅŸematik/gÃ¶rsel okuma)
- **Yeni Model:** Git-Expert (Kimi-K2.5, versiyon kontrol uzmanÄ±)
- **Rule GÃ¼ncellemeleri:**
  - 00-core.md: GLM-5 referanslarÄ±, loop Ã¶nleme referans sadeleÅŸtirme
  - 01-general.md: Model seÃ§im tablosu gÃ¼ncellendi
  - 07-reasoning.md: GLM-5 ve Kimi-K2.5 referanslarÄ±
- **GÃ¼venlik:** Kurumsal referanslar jenerikleÅŸtirildi
- **Premature Close:** Ã‡Ã¶zÃ¼ldÃ¼ (backend kaynaklÄ±)

### v6.2.0 (2026-02-20)
- Premature close debug parametreleri eklendi
- Kimi-K2.5 thinking mode varsayÄ±lan AÃ‡IK
- Temperature optimizasyonu (thinking=1.0)
- Request defaults anchor yapÄ±sÄ±

### v4.4.0 (2026-02-05)
- Atomik Ã‡Ä±ktÄ± ProtokolÃ¼ eklendi
- Loop Ã¶nleme parametreleri gÃ¼ncellendi

### v4.3.0 (2026-02-04)
- Ä°lk aÃ§Ä±k kaynak yapÄ±sÄ±
- Rules hiyerarÅŸisi, 8 rule dosyasÄ±

---

## ğŸ“„ Lisans

Bu proje MIT lisansÄ± altÄ±nda sunulmaktadÄ±r.

---

## ğŸ™ TeÅŸekkÃ¼rler

- [Continue](https://continue.dev) - VS Code AI asistan eklentisi
- [Z.ai](https://z.ai) - GLM-5 modeli
- [Moonshot AI](https://github.com/MoonshotAI) - Kimi-K2.5 modeli
- [vLLM](https://github.com/vllm-project/vllm) - Inference engine
- Lokal LLM topluluklarÄ± (Reddit r/LocalLLaMA, GitHub)
