# Continue AI Configuration - Professional Local LLM Setup

**Version**: 4.3.0  
**Last Updated**: 2026-02-04

Lokal olarak Ã§alÄ±ÅŸan LLM'leri (GLM-4.7, Qwen3) GitHub Copilot seviyesinde kod Ã¼retimi, debugging, refactoring ve dokÃ¼mantasyon iÃ§in optimize eden Continue IDE eklentisi yapÄ±landÄ±rmasÄ±.

---

## ğŸ¯ AmaÃ§

Bu proje, **lokal LLM'lerin profesyonel yazÄ±lÄ±m geliÅŸtirme asistanÄ± olarak kullanÄ±lmasÄ±nÄ±** saÄŸlar:

| Hedef | AÃ§Ä±klama |
|-------|----------|
| **GitHub Copilot Seviyesi** | Kod tamamlama, debugging, refactoring kalitesi |
| **UzmanlÄ±k AlanlarÄ±** | FPGA/RTL, Embedded C, C#/.NET, Python |
| **Mimari DanÄ±ÅŸmanlÄ±k** | Proje analizi, pattern tespiti, iyileÅŸtirme Ã¶nerileri |
| **DokÃ¼mantasyon** | README, Changelog, API docs oluÅŸturma/gÃ¼ncelleme |
| **Tam Dosya Ã‡Ä±ktÄ±sÄ±** | KÄ±smi diff yerine tam dosya yazÄ±mÄ± (API modelleri iÃ§in) |

---

## ğŸ—ï¸ Mimari

### Model YapÄ±landÄ±rmasÄ±

| Model | Rol | Context | KullanÄ±m AlanÄ± |
|-------|-----|---------|----------------|
| **GLM-4.7-FP8** | TÃ¼m uzmanlÄ±k modelleri | 131K | FPGA, Vitis, C#, Python, Docs, Advisor |
| **Qwen3-Next-80B** | Apply-Model | 262K | Deterministik kod birleÅŸtirme |
| **Qwen3-Coder-480B** | Rerank | 131K | Arama sonucu sÄ±ralama |
| **Qwen3-Embedding-8B** | Embed | - | VektÃ¶rleÅŸtirme |

> **Not:** Kimi-K2.5 tool token leakage sorunu nedeniyle ÅŸu an kullanÄ±lmÄ±yor. vLLM PR #28543 merge edildi, gÃ¼ncelleme sonrasÄ± tekrar denenecek.

### Rules HiyerarÅŸisi

```
.continue/rules/
â”œâ”€â”€ 00-core.md          # Temel protokol (alwaysApply)
â”œâ”€â”€ 01-general.md       # Genel mÃ¼hendislik (alwaysApply)
â”œâ”€â”€ 02-fpga.md          # VHDL/Verilog (glob: *.vhd, *.v)
â”œâ”€â”€ 03-vitis.md         # Embedded C (glob: *.c, *.h)
â”œâ”€â”€ 04-csharp.md        # C#/.NET (glob: *.cs, *.xaml)
â”œâ”€â”€ 05-python.md        # Python (glob: *.py)
â”œâ”€â”€ 06-documentation.md # Markdown (glob: *.md)
â””â”€â”€ 07-reasoning.md     # GeliÅŸmiÅŸ akÄ±l yÃ¼rÃ¼tme (alwaysApply)
```

---

## ğŸ”‘ Temel Ã–zellikler

### 1. Tam Dosya YazÄ±mÄ±
API tabanlÄ± modeller kÄ±smi diff'leri gÃ¼venilir uygulayamaz. Bu yapÄ±landÄ±rma:
- Her deÄŸiÅŸiklikte **dosyanÄ±n tamamÄ±nÄ±** yazar
- `...` veya `// existing code` gibi kÄ±saltmalarÄ± **yasaklar**
- Apply-Model ile deterministik birleÅŸtirme saÄŸlar

### 2. UzmanlÄ±k Modelleri
Her alan iÃ§in optimize edilmiÅŸ system message:
- **FPGA-RTL-Engineer**: FSM, sentez, timing, Vivado
- **Embedded-C-Cpp-Vitis**: Zynq, BSP, DMA, ISR
- **CSharp-DotNet-Engineer**: async/await, MVVM, WPF
- **Python-Engineer**: PEP 8, type hints, pathlib

### 3. Testbench AyrÄ±mÄ±
RTL dosyalarÄ±na testbench otomatik **eklenmez**:
- Sadece `module.vhd` istenirse â†’ sadece `module.vhd`
- Testbench istenirse â†’ ayrÄ± `module_tb.vhd` dosyasÄ±

### 4. DokÃ¼mantasyon KorumasÄ±
Markdown dosyalarÄ±nda mevcut iÃ§erik **silinmez**:
- "Ekle" = mevcut + yeni
- Changelog yeni giriÅŸ = en Ã¼ste
- Format (baÅŸlÄ±k, liste, tablo) korunur

### 5. Tekrar YasaÄŸÄ±
Model Ã§Ä±ktÄ±sÄ±nda tekrar Ã¶nleme:
- AynÄ± cÃ¼mle farklÄ± kelimelerle **yasak**
- Loop tespiti ve otomatik durdurma
- ANALIZ â†’ SONUÃ‡ â†’ KOD formatÄ±

### 6. Error Recovery
Ä°ÅŸlem baÅŸarÄ±sÄ±z olursa:
- BaÅŸarÄ±lÄ± adÄ±mÄ± koru
- Alternatif strateji dene
- KullanÄ±cÄ±ya net bilgi ver

---

## ğŸ“ Dosya YapÄ±sÄ±

```
continue-main/
â”œâ”€â”€ config.yaml              # Ana yapÄ±landÄ±rma dosyasÄ±
â”œâ”€â”€ .continue/
â”‚   â””â”€â”€ rules/
â”‚       â”œâ”€â”€ 00-core.md       # Temel agent protokolÃ¼
â”‚       â”œâ”€â”€ 01-general.md    # Genel mÃ¼hendislik kurallarÄ±
â”‚       â”œâ”€â”€ 02-fpga.md       # FPGA/RTL kurallarÄ±
â”‚       â”œâ”€â”€ 03-vitis.md      # Vitis/Embedded C kurallarÄ±
â”‚       â”œâ”€â”€ 04-csharp.md     # C#/.NET kurallarÄ±
â”‚       â”œâ”€â”€ 05-python.md     # Python kurallarÄ±
â”‚       â”œâ”€â”€ 06-documentation.md # DokÃ¼mantasyon kurallarÄ±
â”‚       â””â”€â”€ 07-reasoning.md  # GeliÅŸmiÅŸ akÄ±l yÃ¼rÃ¼tme
â””â”€â”€ README.md                # Bu dosya
```

---

## âš™ï¸ Kurulum

1. **Continue eklentisini** VS Code'a kurun
2. Bu repository'yi klonlayÄ±n:
   ```bash
   git clone <repo-url>
   ```
3. `config.yaml` iÃ§indeki API ayarlarÄ±nÄ± dÃ¼zenleyin:
   ```yaml
   api_config: &api_config
     provider: openai
     apiBase: <your-api-base>
     apiKey: <your-api-key>
   ```
4. `.continue/rules/` klasÃ¶rÃ¼nÃ¼ VS Code workspace'inize kopyalayÄ±n

---

## ğŸ”§ YapÄ±landÄ±rma SeÃ§enekleri

### Temperature AyarlarÄ±

| Model | Temperature | Neden |
|-------|-------------|-------|
| GLM-4.7 (TÃ¼m uzmanlÄ±klar) | 0.3 | Loop Ã¶nleme + yaratÄ±cÄ±lÄ±k dengesi |
| Apply-Model | 0.0 | Deterministik birleÅŸtirme |
| Rerank | 0.1 | TutarlÄ± sÄ±ralama |

### vLLM Extra Parametreleri (Loop Ã–nleme)

| Parametre | DeÄŸer | AÃ§Ä±klama |
|-----------|-------|----------|
| `repetition_penalty` | 1.0 | KAPALI - GLM loop Ã¶nleme iÃ§in zorunlu |
| `min_p` | 0.01 | DÃ¼ÅŸÃ¼k eÅŸik = Ã§eÅŸitlilik |
| `top_k` | 20 | Stabil Ã§Ä±ktÄ± |
| `frequency_penalty` | 0.0 | Kod iÃ§in sÄ±fÄ±r |
| `presence_penalty` | 0.0 | Kod iÃ§in sÄ±fÄ±r |
| `top_p` | 1.0 | TÃ¼m tokenler dahil |

### Timeout AyarlarÄ±

| Model | Timeout | Neden |
|-------|---------|-------|
| Advisor | 600s | BÃ¼yÃ¼k proje analizi |
| FPGA/Vitis/C#/Python/Docs | 400s | Orta karmaÅŸÄ±klÄ±k |
| Kodlama-Uzmani | 300s | HÄ±zlÄ± kod Ã¼retimi |
| Apply-Model | 900s | BÃ¼yÃ¼k dosya birleÅŸtirme |

---

## ï¿½ YaÅŸanan Sorunlar ve Ã‡Ã¶zÃ¼mler

### GLM-4.7 Sonsuz DÃ¼ÅŸÃ¼nme DÃ¶ngÃ¼sÃ¼ (Infinite Thinking Loop)

**Sorun:** GLM-4.7 modeli thinking modunda sonsuz dÃ¶ngÃ¼ye giriyordu. "Hmm, let me think..." gibi dolgu kelimeleri sÃ¼rekli tekrar ediyordu.

**AraÅŸtÄ±rma KaynaklarÄ±:**
- [Reddit r/LocalLLaMA](https://reddit.com/r/LocalLLaMA) - GLM model deneyimleri
- [Unsloth GitHub Issues](https://github.com/unslothai/unsloth) - Fine-tuning sorunlarÄ±
- [Z.ai Community](https://z.ai) - GLM-4 optimizasyon tartÄ±ÅŸmalarÄ±

**Topluluk Bulgular:**
| KullanÄ±cÄ± | Ã–neri | SonuÃ§ |
|-----------|-------|-------|
| u/AfterAte | `repetition_penalty=1.0` (KAPALI) | âœ… Loop durdu |
| u/PANIC_EXCEPTION | `temperature=0.2-0.3` | âœ… Stabil Ã§Ä±ktÄ± |
| Unsloth docs | `min_p=0.01` (default 0.05 loop yapÄ±yor) | âœ… Ã‡eÅŸitlilik arttÄ± |
| Z.ai forum | `top_k=20` + `top_p=1.0` | âœ… Deterministik ama flexible |

**Ã‡Ã¶zÃ¼m (vLLM Backend iÃ§in):**
```yaml
kodlama_extra: &kodlama_extra
  repetition_penalty: 1.0   # KAPALI - #1 loop sebebi
  min_p: 0.01               # DÃ¼ÅŸÃ¼k eÅŸik
  top_k: 20                 # Stabil Ã§Ä±ktÄ±
  frequency_penalty: 0.0
  presence_penalty: 0.0
  top_p: 1.0
```

**Prompt KurallarÄ±:**
```
THINKING KURALLARI (LOOP ONLEME):
- Ayni fikri TEKRAR ETME - bir kez soyle, sonuca gec
- Dolgu kelimeleri KULLANMA (Hmm, Let me think, Wait, I see)
- Cikmazda kalirsan DURDUR ve kullaniciya sor
```

---

### Kimi K2.5 Tool Token SÄ±zÄ±ntÄ±sÄ± (Tool Token Leakage)

**Sorun:** Kimi K2.5 modeli tool call yaparken `<|toolâ–callsâ–begin|>` gibi Ã¶zel tokenler Ã§Ä±ktÄ±ya sÄ±zÄ±yordu.

**AraÅŸtÄ±rma KaynaklarÄ±:**
- [vLLM GitHub Issues](https://github.com/vllm-project/vllm/issues)
- [vLLM Pull Requests](https://github.com/vllm-project/vllm/pulls)

**Bulunan Ã‡Ã¶zÃ¼m:**
- **PR #28543** (KasÄ±m 2025 - MERGED): `KimiK2ToolParser` state machine implementasyonu
- Token leakage'Ä± Ã¶nleyen dÃ¼zgÃ¼n parsing eklendi

**Durum:** vLLM'i gÃ¼ncel versiyona yÃ¼kseltmek gerekiyor. Åu an Kimi K2.5 yerine GLM-4.7 kullanÄ±lÄ±yor.

---

### maxTokens vs contextLength KarmaÅŸasÄ±

**Sorun:** `maxTokens` ve `contextLength` parametrelerinin farkÄ± net deÄŸildi.

**AÃ§Ä±klama:**
| Parametre | Anlam | Ã–rnek |
|-----------|-------|-------|
| `contextLength` | Modelin gÃ¶rebildiÄŸi TOPLAM token | 131K (GLM-4.7) |
| `maxTokens` | Modelin ÃœRETEBÄ°LECEÄÄ° maksimum token | 32K |

**FormÃ¼l:** `contextLength = input_tokens + maxTokens`

**SeÃ§im:** 32K maxTokens = ~3000 satÄ±r kod kapasitesi. Loop Ã¶nleme ile bÃ¼yÃ¼k dosya dengesi.

---

### Apply Model DokÃ¼mantasyon Sorunu

**Sorun:** Apply-Model dokÃ¼man dosyalarÄ±nda (*.md) dÃ¼zgÃ¼n Ã§alÄ±ÅŸmÄ±yordu. Markdown syntax'Ä±nÄ± bozuyordu.

**Ã‡Ã¶zÃ¼m:** Docs-Writer iÃ§in Apply-Model kullanmak yerine **markdown code block** Ã§Ä±ktÄ±sÄ±:
```markdown
\`\`\`markdown
# DokÃ¼man Ä°Ã§eriÄŸi
Tam iÃ§erik buraya...
\`\`\`
```

KullanÄ±cÄ± kopyalayÄ±p dosyaya yapÄ±ÅŸtÄ±rÄ±yor. Bu yÃ¶ntem daha gÃ¼venilir.

---

### single_find_replace GÃ¼venilirlik Sorunu

**Sorun:** `single_find_replace` tool'u bazen yanlÄ±ÅŸ yere yazÄ±yordu veya hiÃ§ Ã§alÄ±ÅŸmÄ±yordu.

**Ã‡Ã¶zÃ¼m:** `edit_existing_file` tercih edildi (TAM DOSYA yazÄ±mÄ±). Kodlama-Uzmani prompt'undan `single_find_replace` kaldÄ±rÄ±ldÄ±.

---

## ğŸ“‹ Versiyon GeÃ§miÅŸi

### v4.3.0 (2026-02-04)
- `kodlama_params` kaldÄ±rÄ±ldÄ± (kullanÄ±lmÄ±yordu, kafa karÄ±ÅŸtÄ±rÄ±yordu)
- **00-core.md** ANALIZ/SONUÃ‡/KOD formatÄ±na dokÃ¼mantasyon muafiyeti eklendi
- README ve config.yaml model/timeout tutarlÄ±lÄ±ÄŸÄ± saÄŸlandÄ±
- General-Engineer referanslarÄ± kaldÄ±rÄ±ldÄ± (mevcut deÄŸil)

### v4.2.0 (2026-02-03)
- **BREAKING:** GLM-4.7 loop prevention parametreleri
  - `repetition_penalty=1.0` (KAPALI)
  - `min_p=0.01`, `top_k=20`
- **THINKING KURALLARI** gÃ¼ncellendi
  - "3 adÄ±m limiti" â†’ "tekrar yasaÄŸÄ±" (model zekasÄ±nÄ± kÄ±sÄ±tlamaz)
  - Dolgu kelimeleri listesi geniÅŸletildi
- **Docs-Writer** Qwen3-30B â†’ GLM-4.7-FP8 geÃ§iÅŸi
- **07-reasoning.md** loop Ã¶nleme protokolÃ¼ eklendi
- **06-documentation.md** markdown code block yÃ¶ntemi

### v4.1.0 (2026-02-02)
- maxTokens 65K â†’ 32K (loop Ã¶nleme dengesi)
- single_find_replace Kodlama-Uzmani'dan kaldÄ±rÄ±ldÄ±
- Git history temizlendi (orphan branch)

### v3.0.3 (2026-02-02)
- Temperature optimizasyonu (GLM-4.7 â†’ 0.15)
- K2.5 bÃ¼yÃ¼k proje analizi eklendi
- Output format kurallarÄ± (ANALIZ-SONUÃ‡-KOD)
- Error recovery protokolÃ¼
- maxTokens 32K'ya optimize edildi

### v3.0.2 (2026-02-01)
- Tekrar yasaÄŸÄ± (TEKRAR YASAGI) eklendi
- Vitis output format kurallarÄ±

### v3.0.1 (2026-02-01)
- Apply model silme operasyonu desteÄŸi
- [DELETE], [REMOVE], [SÄ°L] markers

### v3.0.0 (2026-01-26)
- Tam yeniden yapÄ±landÄ±rma
- Rules hiyerarÅŸisi oluÅŸturuldu
- UzmanlÄ±k modelleri tanÄ±mlandÄ±

---

## ğŸ¤ KatkÄ±da Bulunma

1. Fork edin
2. Feature branch oluÅŸturun (`git checkout -b feature/amazing-feature`)
3. DeÄŸiÅŸikliklerinizi commit edin (`git commit -m 'Add amazing feature'`)
4. Branch'i push edin (`git push origin feature/amazing-feature`)
5. Pull Request aÃ§Ä±n

---

## ğŸ“„ Lisans

Bu proje MIT lisansÄ± altÄ±nda sunulmaktadÄ±r.

---

## ğŸ™ TeÅŸekkÃ¼rler

- [Continue](https://continue.dev) - VS Code AI asistan eklentisi
- Lokal LLM topluluklarÄ±
